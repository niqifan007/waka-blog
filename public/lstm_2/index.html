<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>了解LSTM神经网络 - waka&#39;s blog</title><meta name="author" content="waka">
<meta name="author-link" content="">
<meta name="description" content="了解LSTM神经网络" /><meta name="keywords" content='LSTM,, 神经网络,, 机器学习' /><meta itemprop="name" content="了解LSTM神经网络">
<meta itemprop="description" content="了解LSTM神经网络"><meta itemprop="datePublished" content="2023-04-24T01:44:51+08:00" />
<meta itemprop="dateModified" content="2023-04-24T01:44:51+08:00" />
<meta itemprop="wordCount" content="3792">
<meta itemprop="keywords" content="LSTM,神经网络,机器学习," /><meta property="og:title" content="了解LSTM神经网络" />
<meta property="og:description" content="了解LSTM神经网络" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://blog.579878700.xyz/lstm_2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-24T01:44:51+08:00" />
<meta property="article:modified_time" content="2023-04-24T01:44:51+08:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="了解LSTM神经网络"/>
<meta name="twitter:description" content="了解LSTM神经网络"/>
<meta name="application-name" content="waka&#39;s blog">
<meta name="apple-mobile-web-app-title" content="waka&#39;s blog"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="%e7%8b%90%e7%8b%b8.svg"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://blog.579878700.xyz/lstm_2/" /><link rel="prev" href="http://blog.579878700.xyz/lstm/" /><link rel="next" href="http://blog.579878700.xyz/dockerhub/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "了解LSTM神经网络",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http:\/\/blog.579878700.xyz\/lstm_2\/"
    },"genre": "posts","keywords": "LSTM, 神经网络, 机器学习","wordcount":  3792 ,
    "url": "http:\/\/blog.579878700.xyz\/lstm_2\/","datePublished": "2023-04-24T01:44:51+08:00","dateModified": "2023-04-24T01:44:51+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "waka"
      },"description": "了解LSTM神经网络"
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="waka&#39;s blog"><img loading="lazy" src="/%e7%8b%90%e7%8b%b8.svg" srcset="/%e7%8b%90%e7%8b%b8.svg, /%e7%8b%90%e7%8b%b8.svg 1.5x, /%e7%8b%90%e7%8b%b8.svg 2x" sizes="auto" data-title="waka&#39;s blog" data-alt="waka&#39;s blog" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/><span class="header-title-text">waka&#39;s blog</span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容……" id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="waka&#39;s blog"><img loading="lazy" src="/%e7%8b%90%e7%8b%b8.svg" srcset="/%e7%8b%90%e7%8b%b8.svg, /%e7%8b%90%e7%8b%b8.svg 1.5x, /%e7%8b%90%e7%8b%b8.svg 2x" sizes="auto" data-title="/狐狸.svg" data-alt="/狐狸.svg" class="logo" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/><span class="header-title-text">waka&#39;s blog</span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容……" id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    </aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>了解LSTM神经网络</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><img loading="lazy" src="/images/avatar.jpg" srcset="/images/avatar.jpg, /images/avatar.jpg 1.5x, /images/avatar.jpg 2x" sizes="auto" data-title="waka" data-alt="waka" class="avatar" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/>&nbsp;waka</span></span>
          <span class="post-category">收录于 <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><i class="fa-regular fa-folder fa-fw" aria-hidden="true"></i> 机器学习</a></span></div>
      <div class="post-meta-line"><span title=2023-04-24&#32;01:44:51><i class="fa-regular fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2023-04-24">2023-04-24</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i> 约 3792 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw" aria-hidden="true"></i> 预计阅读 8 分钟</span>&nbsp;<span id="/lstm_2/" class="comment-visitors" data-flag-title="了解LSTM神经网络">
              <i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="twikoo_visitors">-</span>&nbsp;次阅读
            </span>&nbsp;<span id="/lstm_2/" class="comment-count" data-flag-title="了解LSTM神经网络">
              <i class="fa-regular fa-comments fa-fw" aria-hidden="true"></i>&nbsp;<span id="twikoo-comment-count">-</span>&nbsp;条评论
            </span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#循环神经网络">循环神经网络</a></li>
    <li><a href="#长期依赖的问题">长期依赖的问题</a></li>
    <li><a href="#长短期记忆网络">长短期记忆网络</a></li>
    <li><a href="#lstm-背后的核心思想">LSTM 背后的核心思想</a></li>
    <li><a href="#循序渐进的-lstm-演练">循序渐进的 LSTM 演练</a></li>
    <li><a href="#长短期记忆的变体">长短期记忆的变体</a></li>
    <li><a href="#结论">结论</a></li>
  </ul>
</nav></div>
      </div><div class="content" id="content"><h2 id="循环神经网络">循环神经网络</h2>
<p>人类不会每一秒都从头开始思考。当你阅读这篇文章时，你会根据你对前面单词的理解来理解每个单词。你不会扔掉所有东西，然后重新开始思考。你的思想有坚持。</p>
<p>传统的神经网络无法做到这一点，这似乎是一个主要缺点。例如，假设您想对电影中每一点发生的事件类型进行分类。目前尚不清楚传统的神经网络如何利用其对电影中先前事件的推理来通知后来的事件。</p>
<p>递归神经网络解决了这个问题。它们是带有循环的网络，允许信息持续存在。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/RNN-rolled.png" srcset="https://s1.imagehub.cc/images/2023/07/25/RNN-rolled.png, https://s1.imagehub.cc/images/2023/07/25/RNN-rolled.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/RNN-rolled.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/RNN-rolled.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/RNN-rolled.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p><strong>递归神经网络有循环。</strong></p>
<p>在上图中，一大块神经网络，$A$, 查看一些输入$x_t$并输出一个值$h_t$. 循环允许信息从网络的一个步骤传递到下一个步骤。</p>
<p>这些循环使递归神经网络看起来有点神秘。然而，如果你多想想，就会发现它们与普通的神经网络并没有什么不同。循环神经网络可以被认为是同一网络的多个副本，每个副本都将一条消息传递给后继者。考虑一下如果我们展开循环会发生什么：</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/RNN-unrolled.png" srcset="https://s1.imagehub.cc/images/2023/07/25/RNN-unrolled.png, https://s1.imagehub.cc/images/2023/07/25/RNN-unrolled.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/RNN-unrolled.png 2x" sizes="auto" data-title="展开的递归神经网络。" data-alt="展开的递归神经网络。" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p><strong>展开的递归神经网络。</strong></p>
<p>这种链状性质表明循环神经网络与序列和列表密切相关。它们是用于此类数据的神经网络的自然架构。</p>
<p>他们肯定被使用了！在过去的几年里，将 RNN 应用于各种问题取得了令人难以置信的成功：语音识别、语言建模、翻译、图像字幕……这个例子不胜枚举。关于 RNN 可以实现的惊人壮举，我将在 Andrej Karpathy 的优秀博客文章<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"target="_blank" rel="external nofollow noopener noreferrer">The Unreasonable Effectiveness of Recurrent Neural Networks<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中进行讨论。但他们真的很了不起。</p>
<p>这些成功的关键是使用“LSTM”，这是一种非常特殊的递归神经网络，对于许多任务，它比标准版本要好得多。几乎所有基于递归神经网络的令人兴奋的结果都是用它们实现的。本文将探讨这些 LSTM。</p>
<h2 id="长期依赖的问题">长期依赖的问题</h2>
<p>RNN 的吸引力之一是它们可能能够将以前的信息连接到当前的任务，例如使用以前的视频帧可能会告知对当前帧的理解。如果 RNN 可以做到这一点，它们将非常有用。但他们可以吗？这取决于。</p>
<p>有时，我们只需要查看最近的信息即可执行当前任务。例如，考虑一个语言模型试图根据前面的单词预测下一个单词。如果我们试图预测“the clouds are in the <em>sky</em> ”中的最后一个词，我们不需要任何进一步的上下文——很明显下一个词将是 sky。在这种情况下，相关信息和需要它的地方之间的差距很小，RNN 可以学习使用过去的信息。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/RNN-shorttermdepdencies.png" srcset="https://s1.imagehub.cc/images/2023/07/25/RNN-shorttermdepdencies.png, https://s1.imagehub.cc/images/2023/07/25/RNN-shorttermdepdencies.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/RNN-shorttermdepdencies.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/RNN-shorttermdepdencies.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/RNN-shorttermdepdencies.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>但在某些情况下，我们需要更多上下文。考虑尝试预测文本“我在法国长大……我说一口流利的_法语_”中的最后一个词。最近的信息表明，下一个词可能是一种语言的名称，但如果我们想缩小哪种语言的范围，我们需要法国的上下文，从更远的地方。相关信息和需要它的点之间的差距变得非常大是完全有可能的。</p>
<p>不幸的是，随着差距的扩大，RNN 变得无法学习连接信息。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/RNN-longtermdependencies.png" srcset="https://s1.imagehub.cc/images/2023/07/25/RNN-longtermdependencies.png, https://s1.imagehub.cc/images/2023/07/25/RNN-longtermdependencies.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/RNN-longtermdependencies.png 2x" sizes="auto" data-title="神经网络与长期依赖性作斗争。" data-alt="神经网络与长期依赖性作斗争。" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>理论上，RNN 绝对有能力处理这种“长期依赖”。人类可以仔细地为他们挑选参数来解决这种形式的玩具问题。遗憾的是，在实践中，RNN 似乎无法学习它们。<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf"target="_blank" rel="external nofollow noopener noreferrer">Hochreiter (1991) [German]<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">Bengio 等人<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>深入探讨了这个问题。<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">(1994)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，他发现了一些非常根本的原因，为什么这可能很困难。</p>
<p>值得庆幸的是，LSTM 没有这个问题！</p>
<h2 id="长短期记忆网络">长短期记忆网络</h2>
<p>长短期记忆网络——通常简称为“LSTM”——是一种特殊的 RNN，能够学习长期依赖关系。<a href="http://www.bioinf.jku.at/publications/older/2604.pdf"target="_blank" rel="external nofollow noopener noreferrer">它们由Hochreiter &amp; Schmidhuber (1997)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>引入，并在后续工作中被许多人提炼和推广。<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1"target="_blank" rel="external nofollow noopener noreferrer"><sup><span><span>1</span></span></sup><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>它们在处理大量问题时表现出色，现在已被广泛使用。</p>
<p>LSTM 明确设计用于避免长期依赖问题。长时间记住信息实际上是他们的默认行为，而不是他们努力学习的东西！</p>
<p>所有循环神经网络都具有神经网络重复模块链的形式。在标准 RNN 中，这个重复模块将具有非常简单的结构，例如单个 tanh 层。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-SimpleRNN.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-SimpleRNN.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-SimpleRNN.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-SimpleRNN.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM3-SimpleRNN.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM3-SimpleRNN.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p><strong>标准 RNN 中的重复模块包含单层。</strong></p>
<p>LSTM 也有这种链状结构，但重复模块有不同的结构。不是只有一个神经网络层，而是有四个，以一种非常特殊的方式进行交互。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-chain.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-chain.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-chain.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-chain.png 2x" sizes="auto" data-title="LSTM 神经网络。" data-alt="LSTM 神经网络。" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p><strong>LSTM 中的重复模块包含四个交互层。</strong></p>
<p>不要担心正在发生的事情的细节。稍后我们将逐步介绍 LSTM 图。现在，让我们试着熟悉我们将要使用的符号。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM2-notation.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM2-notation.png, https://s1.imagehub.cc/images/2023/07/25/LSTM2-notation.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM2-notation.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM2-notation.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM2-notation.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>在上图中，每条线都带有一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈代表逐点操作，如向量加法，而黄色框是学习的神经网络层。行合并表示串联，而行分叉表示其内容被复制并且副本转到不同的位置。</p>
<h2 id="lstm-背后的核心思想">LSTM 背后的核心思想</h2>
<p>LSTM 的关键是细胞状态，即贯穿图表顶部的水平线。</p>
<p>细胞状态有点像传送带。它直接沿着整个链条运行，只有一些次要的线性交互。信息很容易原封不动地沿着它流动。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-C-line.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-C-line.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-C-line.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-C-line.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM3-C-line.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM3-C-line.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>LSTM 确实有能力删除或添加信息到细胞状态，由称为门的结构仔细调节。</p>
<p>门是一种有选择地让信息通过的方式。它们由一个 sigmoid 神经网络层和一个逐点乘法运算组成。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-gate.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-gate.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-gate.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-gate.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM3-gate.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM3-gate.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>sigmoid 层输出介于 0 和 1 之间的数字，描述应该让多少成分通过。值为零表示“不让任何东西通过”，而值为 1 表示“让一切都通过！”</p>
<p>LSTM 具有三个这样的门，以保护和控制单元状态。</p>
<h2 id="循序渐进的-lstm-演练">循序渐进的 LSTM 演练</h2>
<p>LSTM 的第一步是决定我们要从细胞状态中丢弃哪些信息。该决定由称为“遗忘门层”的 S 形层做出。它看着$h_{t-1}$和$x_t$, 并输出一个介于$0$和$1$对于细胞状态中的每个数字$C_{t-1}$. A$1$代表“完全保留这个”，而一个$0$代表“彻底摆脱这个”。</p>
<p>让我们回到我们的语言模型示例，它试图根据所有先前的单词预测下一个单词。在这样的问题中，细胞状态可能包括当前主体的性别，以便可以使用正确的代词。当我们看到一个新主题时，我们想忘记旧主题的性别。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-f.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-f.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-f.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-f.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-f.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-f.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>下一步是决定我们要在细胞状态中存储哪些新信息。这有两个部分。首先，称为“输入门层”的 sigmoid 层决定我们将更新哪些值。接下来，一个 tanh 层创建一个新候选值的向量，$\tilde{C}_t$，可以将其添加到状态。在下一步中，我们将结合这两者来创建对状态的更新。</p>
<p>在我们的语言模型示例中，我们想要将新主题的性别添加到单元格状态，以替换我们忘记的旧主题。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-i.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-i.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-i.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-i.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-i.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-i.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>现在是更新旧细胞状态的时候了，$C_{t-1}$, 进入新的细胞状态$C_t$. 前面的步骤已经决定了要做什么，我们只需要实际去做。</p>
<p>我们将旧状态乘以$f_t$，忘记了我们早些时候决定忘记的事情。然后我们添加$i_t*\tilde{C}_t$. 这是新的候选值，根据我们决定更新每个状态值的程度进行缩放。</p>
<p>在语言模型的情况下，这是我们实际上要删除有关旧主题性别的信息并添加新信息的地方，正如我们在前面的步骤中所决定的那样。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-C.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-C.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-C.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-C.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-C.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-C.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>最后，我们需要决定要输出什么。此输出将基于我们的细胞状态，但将是过滤后的版本。首先，我们运行一个 sigmoid 层，它决定我们要输出细胞状态的哪些部分。然后，我们将细胞状态通过$\tanh$（将值推到介于$-1$和$1$) 并将其乘以 sigmoid 门的输出，这样我们就只输出我们决定输出的部分。</p>
<p>对于语言模型示例，由于它刚刚看到一个主语，它可能想要输出与动词相关的信息，以防接下来会发生什么。例如，它可能会输出主语是单数还是复数，以便我们知道如果接下来是动词应该变位的形式。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-o.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-o.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-o.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-o.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-o.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM3-focus-o.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<h2 id="长短期记忆的变体">长短期记忆的变体</h2>
<p>到目前为止，我所描述的是一个非常普通的 LSTM。但并不是所有的 LSTM 都和上面的一样。事实上，似乎几乎每篇涉及 LSTM 的论文都使用略有不同的版本。差异很小，但值得一提的是其中的一些差异。</p>
<p><a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf"target="_blank" rel="external nofollow noopener noreferrer">由Gers &amp; Schmidhuber (2000)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>引入的一种流行的 LSTM 变体正在添加“窥孔连接”。这意味着我们让门层查看单元状态。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-peepholes.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-peepholes.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-peepholes.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-peepholes.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-peepholes.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-peepholes.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>上图给所有的门都加了窥视孔，但是很多论文会给一些窥视孔，其他的就不给了。</p>
<p>另一种变体是使用耦合的遗忘门和输入门。我们不是单独决定要忘记什么以及我们应该向什么添加新信息，而是一起做出这些决定。我们只会忘记什么时候要输入一些东西来代替它。当我们忘记旧的东西时，我们只会向状态输入新值。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-tied.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-tied.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-tied.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-tied.png 2x" sizes="auto" data-title="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-tied.png" data-alt="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-tied.png" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>LSTM 的一个稍微更显着的变化是门控循环单元 (Gated Recurrent Unit, GRU)，由<a href="http://arxiv.org/pdf/1406.1078v3.pdf"target="_blank" rel="external nofollow noopener noreferrer">Cho 等人引入。(2014)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它将遗忘门和输入门组合成一个“更新门”。它还合并了细胞状态和隐藏状态，并做了一些其他的改变。生成的模型比标准 LSTM 模型更简单，并且越来越受欢迎。</p>
<p><img loading="lazy" src="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-GRU.png" srcset="https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-GRU.png, https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-GRU.png 1.5x, https://s1.imagehub.cc/images/2023/07/25/LSTM3-var-GRU.png 2x" sizes="auto" data-title="门控循环单元神经网络。" data-alt="门控循环单元神经网络。" style="background: url(/svg/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;this.alt=this.dataset.alt;for(const a of ['style','data-title','data-alt','onerror','onload']){this.removeAttribute(a);}"/></p>
<p>这些只是一些最著名的 LSTM 变体。还有很多其他的，比如<a href="http://arxiv.org/pdf/1508.03790v2.pdf"target="_blank" rel="external nofollow noopener noreferrer">Yao 等人的 Depth Gated RNNs。(2015)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。还有一些完全不同的方法来解决长期依赖关系，比如<a href="http://arxiv.org/pdf/1402.3511v1.pdf"target="_blank" rel="external nofollow noopener noreferrer">Koutnik 等人的 Clockwork RNNs。(2014)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>这些变体中哪个最好？差异重要吗？<a href="http://arxiv.org/pdf/1503.04069.pdf"target="_blank" rel="external nofollow noopener noreferrer">格雷夫等人。(2015)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>对流行的变体进行了很好的比较，发现它们都差不多。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf"target="_blank" rel="external nofollow noopener noreferrer">Jozefowicz 等人。(2015)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>测试了超过一万个 RNN 架构，发现其中一些在某些任务上比 LSTM 更有效。</p>
<h2 id="结论">结论</h2>
<p>早些时候，我提到了人们使用 RNN 取得的显著成果。基本上所有这些都是使用 LSTM 实现的。对于大多数任务，它们确实工作得更好！</p>
<p>写成一组方程式，LSTM 看起来相当吓人。希望在本文中逐步介绍它们可以使它们更容易理解。</p>
<p>LSTM 是我们使用 RNN 可以完成的一大步。人们很自然地会想：是否又迈出了一大步？研究人员的共同意见是：“是的！还有下一步，就是注意！这个想法是让 RNN 的每一步都从一些更大的信息集合中挑选信息来查看。例如，如果您使用 RNN 创建描述图像的标题，它可能会选择图像的一部分来查看它输出的每个单词。事实上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf"target="_blank" rel="external nofollow noopener noreferrer">徐_等人。_(2015)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>正是这样做的——如果你想探索注意力，这可能是一个有趣的起点！使用注意力已经取得了许多非常令人兴奋的结果，而且似乎还有更多的结果……</p>
<p>注意力并不是 RNN 研究中唯一令人兴奋的话题。例如，<a href="http://arxiv.org/pdf/1507.01526v1.pdf"target="_blank" rel="external nofollow noopener noreferrer">Kalchbrenner等_人的 Grid LSTM。_(2015)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>似乎非常有前途。在生成模型中使用 RNN——例如<a href="http://arxiv.org/pdf/1502.04623.pdf"target="_blank" rel="external nofollow noopener noreferrer">Gregor_等人。_(2015)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> ,<a href="http://arxiv.org/pdf/1506.02216v3.pdf"target="_blank" rel="external nofollow noopener noreferrer">钟_等人。_(2015)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>或<a href="http://arxiv.org/pdf/1411.7610v3.pdf"target="_blank" rel="external nofollow noopener noreferrer">Bayer &amp; Osendorfer (2015)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> – 似乎也很有趣。过去几年对于递归神经网络来说是一个激动人心的时期，而即将到来的只会更加精彩！</p></div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-04-24&#32;01:44:51>更新于 2023-04-24&nbsp;</span>
      </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/lstm_2/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="http://blog.579878700.xyz/lstm_2/" data-title="了解LSTM神经网络" data-hashtags="LSTM,神经网络,机器学习"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="http://blog.579878700.xyz/lstm_2/" data-hashtag="LSTM"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="http://blog.579878700.xyz/lstm_2/" data-title="了解LSTM神经网络"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw me-1" aria-hidden="true"></i><a href='/tags/lstm/' class="post-tag">LSTM</a><a href='/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/' class="post-tag">神经网络</a><a href='/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/' class="post-tag">机器学习</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/lstm/" class="post-nav-item" rel="prev" title="使用LSTM对航班延误进行预测"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>使用LSTM对航班延误进行预测</a>
      <a href="/dockerhub/" class="post-nav-item" rel="next" title="Docker Hub 镜像加速器汇总">Docker Hub 镜像加速器汇总<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="twikoo"></div><noscript>
        Please enable JavaScript to view the comments powered by <a href="https://twikoo.js.org/" rel="external nofollow noopener noreferrer">Twikoo</a>.
      </noscript></div></article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.111.1">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.18-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2023</span><span class="author" itemprop="copyrightHolder">
              <a href="/">waka</a></span></div><div class="footer-line statistics"></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div><div class="fixed-button view-comments d-none" role="button" aria-label="查看评论"><i class="fa-solid fa-comment fa-fw" aria-hidden="true"></i></div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/twikoo/twikoo.all.min.js" defer></script><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/lunr/lunr.min.js" defer></script><script src="/lib/lunr/lunr.stemmer.support.min.js" defer></script><script src="/lib/lunr/lunr.zh.min.js" defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":true,"expired":false,"twikoo":{"commentCount":true,"el":"#twikoo","envId":"https://twikoo.579878700.xyz/","lang":"zh-cn"}},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"enablePWA":true,"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":6,"speed":100}};</script><script src="/js/theme.min.js" defer></script></body>
</html>
